import shutil
import warnings

from pathlib import Path

import click
import numpy as np
import pandas as pd
import submitit

from iohub import open_ome_zarr

from mantis.analysis.AnalysisSettings import StitchSettings
from mantis.analysis.stitch import (
    get_grid_rows_cols,
    get_image_shift,
    get_stitch_output_shape,
    preprocess_and_shift,
    stitch_shifted_store,
)
from mantis.cli.parsing import config_filepath, input_position_dirpaths, output_dirpath
from mantis.cli.utils import create_empty_hcs_zarr, process_single_position_v2, yaml_to_model

HAS_SLURM = True


def batched(iterable, n=1):
    length = len(iterable)
    for ndx in range(0, length, n):
        yield iterable[ndx : min(ndx + n, length)]


@click.command()
@input_position_dirpaths()
@output_dirpath()
@config_filepath()
@click.option(
    "--temp-path",
    type=click.Path(exists=True, file_okay=False, dir_okay=True),
    default='./',
    help="Path to temporary directory, ideally with fast read/write speeds, e.g. /hpc/scratch/group.comp.micro/",
)
@click.option("--debug", is_flag=True, help="Run in debug mode")
def stitch(
    input_position_dirpaths: list[Path],
    output_dirpath: str,
    config_filepath: str,
    temp_path: str,
    debug: bool,
) -> None:
    """
    Stitch positions in wells of a zarr store using a configuration file generated by estimate-stitch.

    >>> mantis stitch -i ./input.zarr/*/*/* -c ./stitch_params.yml -o ./output.zarr --temp-path /hpc/scratch/group.comp.micro/
    """
    if not HAS_SLURM:
        warnings.warn(
            "This function is intended to be used with SLURM. "
            "Running on local machine instead."
        )
    assert not Path(output_dirpath).exists(), f'Output path: {output_dirpath} already exists'

    slurm_out_path = Path(output_dirpath).parent / "slurm_output"
    shifted_store_path = Path(temp_path, f"TEMP_{input_position_dirpaths[0].parts[-4]}")
    settings = yaml_to_model(config_filepath, StitchSettings)

    with open_ome_zarr(str(input_position_dirpaths[0]), mode="r") as input_dataset:
        input_dataset_channels = input_dataset.channel_names
        T, C, Z, Y, X = input_dataset.data.shape
        scale = tuple(input_dataset.scale)
        chunks = input_dataset.data.chunks

    if settings.channels is None:
        settings.channels = input_dataset_channels

    assert all(
        channel in input_dataset_channels for channel in settings.channels
    ), "Invalid channel(s) provided."

    wells = list(set([Path(*p.parts[-3:-1]) for p in input_position_dirpaths]))
    fov_names = set([p.name for p in input_position_dirpaths])
    grid_rows, grid_cols = get_grid_rows_cols(fov_names)
    n_rows = len(grid_rows)
    n_cols = len(grid_cols)

    if settings.total_translation is None:
        output_shape, global_translation = get_stitch_output_shape(
            n_rows, n_cols, Y, X, settings.column_translation, settings.row_translation
        )
    else:
        df = pd.DataFrame.from_dict(
            settings.total_translation, orient="index", columns=["shift-y", "shift-x"]
        )
        output_shape = (
            np.ceil(df["shift-y"].max() + Y).astype(int),
            np.ceil(df["shift-x"].max() + X).astype(int),
        )

    # create temp zarr store
    click.echo(f'Creating temporary zarr store at {shifted_store_path}')
    stitched_shape = (T, len(settings.channels), Z) + output_shape
    stitched_chunks = chunks[:3] + (4096, 4096)
    with open_ome_zarr(
        shifted_store_path, layout='hcs', mode='w', channel_names=settings.channels
    ) as temp_dataset:
        for position_path in input_position_dirpaths:
            position = temp_dataset.create_position(*position_path.parts[-3:])
            position.create_zeros(
                name='0',
                shape=stitched_shape,
                chunks=stitched_chunks,
                dtype=np.float32,
            )

    slurm_args = {
        "slurm_mem_per_cpu": "24G",
        "slurm_cpus_per_task": 6,
        "slurm_array_parallelism": 100,  # only 100 jobs can run at the same time
        "slurm_time": 30,
        "slurm_job_name": "shift",
        "slurm_partition": "preempted",
    }

    executor = submitit.AutoExecutor(folder=slurm_out_path)
    executor.update_parameters(**slurm_args)

    click.echo('Submitting SLURM jobs')
    shift_jobs = []

    with executor.batch():
        for in_path in input_position_dirpaths:
            well = Path(*in_path.parts[-3:-1])
            col, row = (in_path.name[:3], in_path.name[3:])

            if settings.total_translation is None:
                shift = get_image_shift(
                    int(col),
                    int(row),
                    settings.column_translation,
                    settings.row_translation,
                    global_translation,
                )
            else:
                # COL+ROW order here is important
                shift = settings.total_translation[str(well / (col + row))]

            job = executor.submit(
                process_single_position_v2,
                preprocess_and_shift,
                input_channel_idx=[
                    input_dataset_channels.index(ch) for ch in settings.channels
                ],
                output_channel_idx=list(range(len(settings.channels))),
                time_indices='all',
                num_processes=6,
                settings=settings.preprocessing,
                output_shape=output_shape,
                verbose=True,
                shift_x=float(shift[-1]),
                shift_y=float(shift[-2]),
                input_data_path=in_path,
                output_path=shifted_store_path,
            )
            shift_jobs.append(job)

    shift_job_ids = [job.job_id for job in shift_jobs]

    # create output zarr store
    create_empty_hcs_zarr(
        store_path=output_dirpath,
        position_keys=[Path(well, '0').parts for well in wells],
        channel_names=settings.channels,
        shape=stitched_shape,
        chunks=stitched_chunks,
        scale=scale,
    )

    slurm_args = {
        "slurm_mem_per_cpu": "8G",
        "slurm_cpus_per_task": 32,
        "slurm_time": "1-00:00:00",  # in [DD-]HH:MM:SS format
        "slurm_partition": "cpu",
        "slurm_job_name": "stitch",
        "slurm_dependency": f"afterok:{shift_job_ids[0]}:{shift_job_ids[-1]}",
    }

    executor = submitit.AutoExecutor(folder=slurm_out_path)
    executor.update_parameters(**slurm_args)

    stitch_job = executor.submit(
        stitch_shifted_store,
        shifted_store_path,
        output_dirpath,
        settings.postprocessing,
        blending='average',
        verbose=True,
    )

    if not debug:
        slurm_args = {
            "slurm_partition": "cpu",
            "slurm_mem_per_cpu": "12G",
            "slurm_cpus_per_task": 1,
            "slurm_time": "0-01:00:00",  # in [DD-]HH:MM:SS format
            "slurm_job_name": "cleanup",
            "slurm_dependency": f"afterok:{stitch_job}",
        }
        executor = submitit.AutoExecutor(folder=slurm_out_path)
        executor.update_parameters(**slurm_args)
        executor.submit(shutil.rmtree, shifted_store_path)


if __name__ == '__main__':
    stitch()
